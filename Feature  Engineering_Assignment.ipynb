{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee68386-12be-4338-a72b-999844eb570f",
   "metadata": {},
   "source": [
    "Q1.What is a parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ca4190-3840-40d3-a985-9dc22cc08447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a parameter is a variable that is used to define a particular characteristic or feature of a dataset.\\nParameters are often used to control the behavior of algorithms and models, and they can be adjusted to optimize \\nthe performance of a machine learning model. For example, in a linear regression model, the parameters are the coefficients \\nthat determine the relationship between the input features and the target variable. By tuning these parameters, \\n can improve the accuracy and effectiveness of model.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"a parameter is a variable that is used to define a particular characteristic or feature of a dataset.\n",
    "Parameters are often used to control the behavior of algorithms and models, and they can be adjusted to optimize \n",
    "the performance of a machine learning model. For example, in a linear regression model, the parameters are the coefficients \n",
    "that determine the relationship between the input features and the target variable. By tuning these parameters, \n",
    " can improve the accuracy and effectiveness of model.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e9bb7-138e-4875-8036-4567f816edb0",
   "metadata": {},
   "source": [
    "Q2.What is correlation? What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fa1208-bf95-4468-8052-637f1d8a7185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A negative correlation means that as one variable increases, the other decreases. \\nFor example, if the number of hours spent watching TV increases, the number of hours \\nspent studying might decrease, indicating a negative correlation between TV watching and studying'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Correlation measures the relationship between two variables. \n",
    "It indicates how one variable changes in response to changes in another variable.\n",
    "The correlation coefficient ranges from -1 to 1\"\"\"\n",
    "\n",
    "\"\"\"A negative correlation means that as one variable increases, the other decreases. \n",
    "For example, if the number of hours spent watching TV increases, the number of hours \n",
    "spent studying might decrease, indicating a negative correlation between TV watching and studying\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc55a-95c0-4dd5-856d-fd43d0fa430b",
   "metadata": {},
   "source": [
    "Q3. Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b307b3-a212-4983-a3e9-72d8cb21cf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Machine Learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn \\nand improve from experience without being explicitly programmed. It involves the development of \\nalgorithms that can analyze and interpret data, identify patterns, and make decisions based on that data.\\n\\nMain Components:\\n\\nData: The raw information used for training and testing.\\n\\nFeatures: Input variables used to make predictions.\\n\\nModel: The algorithm that learns from the data.\\n\\nTraining: The process of teaching the model using data.\\n\\nEvaluation: Assessing the model's performance.\\n\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Machine Learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn \n",
    "and improve from experience without being explicitly programmed. It involves the development of \n",
    "algorithms that can analyze and interpret data, identify patterns, and make decisions based on that data.\n",
    "\n",
    "Main Components:\n",
    "\n",
    "Data: The raw information used for training and testing.\n",
    "\n",
    "Features: Input variables used to make predictions.\n",
    "\n",
    "Model: The algorithm that learns from the data.\n",
    "\n",
    "Training: The process of teaching the model using data.\n",
    "\n",
    "Evaluation: Assessing the model's performance.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca3bfc-6e6a-4dc0-8546-ede8deccc7ad",
   "metadata": {},
   "source": [
    " Q4.How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f68a471-61e5-42a4-a539-216acdbb53ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The loss value, also known as the cost or error, measures how well a machine learning model's \\npredictions match the actual data. Here's how it helps in determining the model's quality:\\n\\nIndicator of Performance: A lower loss value indicates that the model's predictions are closer to\\nthe actual values, suggesting better performance.\\n\\nGuides Optimization: During training, the model adjusts its parameters to minimize the loss value,\\nimproving its accuracy.\\n\\nComparison Tool: By comparing loss values across different models or iterations, can identify which model performs best.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The loss value, also known as the cost or error, measures how well a machine learning model's \n",
    "predictions match the actual data. Here's how it helps in determining the model's quality:\n",
    "\n",
    "Indicator of Performance: A lower loss value indicates that the model's predictions are closer to\n",
    "the actual values, suggesting better performance.\n",
    "\n",
    "Guides Optimization: During training, the model adjusts its parameters to minimize the loss value,\n",
    "improving its accuracy.\n",
    "\n",
    "Comparison Tool: By comparing loss values across different models or iterations, can identify which model performs best.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25e8a2-b863-494c-ae1a-22d9cbc530f6",
   "metadata": {},
   "source": [
    "Q5.What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c44f5cf0-69e7-4916-9239-33a8d724d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Continuous Variables: These are variables that can take any value within a given range. \\nThey are typically numerical and can be measured. Examples include height, weight, temperature, and time. \\nContinuous variables can have an infinite number of possible values.\\n\\nCategorical Variables: These are variables that represent distinct categories or groups. \\nThey are often non-numerical and can be divided into a limited number of categories. \\nExamples include gender, color, type of car, and brand of a product. Categorical variables \\ncan be further divided into:\\n\\nNominal Variables: Categories without any inherent order (e.g., colors: red, blue, green).\\n\\nOrdinal Variables: Categories with a specific order (e.g., rankings: first, second, third).'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Continuous Variables: These are variables that can take any value within a given range. \n",
    "They are typically numerical and can be measured. Examples include height, weight, temperature, and time. \n",
    "Continuous variables can have an infinite number of possible values.\n",
    "\n",
    "Categorical Variables: These are variables that represent distinct categories or groups. \n",
    "They are often non-numerical and can be divided into a limited number of categories. \n",
    "Examples include gender, color, type of car, and brand of a product. Categorical variables \n",
    "can be further divided into:\n",
    "\n",
    "Nominal Variables: Categories without any inherent order (e.g., colors: red, blue, green).\n",
    "\n",
    "Ordinal Variables: Categories with a specific order (e.g., rankings: first, second, third).'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b843a8be-9203-4f0a-99f3-d673aa487917",
   "metadata": {},
   "source": [
    "Q6.How do we handle categorical variables in Machine Learning? What are the common t\n",
    " echniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51f5347c-e3d7-4d78-b45f-3084b34fdc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do we handle categorical variables in Machine Learning? What are the common t echniques?\\nHandling categorical variables is crucial in machine learning, as many algorithms require numerical input.\\nHere are some common techniques:\\n\\nLabel Encoding: Converts categories into numerical labels. \\nFor example, \"red\" becomes 1, \"blue\" becomes 2, and so on.\\nThis method is simple but can introduce ordinal relationships where none exist.\\n\\nOne-Hot Encoding: Creates binary columns for each category. \\nFor example, a \"color\" variable with values \"red,\" \"blue,\" and \"green\" would be transformed\\ninto three columns: \"color_red,\" \"color_blue,\" and \"color_green,\" with binary values indicating the presence of each color.\\n\\nOrdinal Encoding: Similar to label encoding but used when the categories have a meaningful order.\\nFor example, \"low,\" \"medium,\" and \"high\" could be encoded as 1, 2, and 3, respectively.\\n\\nBinary Encoding: Combines label encoding and one-hot encoding. Each category is first converted to a binary number, and then each digit becomes a separate column. This method can be more memory-efficient than one-hot encoding.\\n\\nFrequency Encoding: Replaces categories with their frequency of occurrence. \\nFor example, if \"red\" appears 50 times, \"blue\" 30 times, and \"green\" 20 times, they would be encoded as 50, 30, and 20, respectively.\\n\\nTarget Encoding: Replaces categories with the mean of the target variable for each category. \\nThis method can be powerful but may lead to overfitting if not used carefully.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''How do we handle categorical variables in Machine Learning? What are the common t echniques?\n",
    "Handling categorical variables is crucial in machine learning, as many algorithms require numerical input.\n",
    "Here are some common techniques:\n",
    "\n",
    "Label Encoding: Converts categories into numerical labels. \n",
    "For example, \"red\" becomes 1, \"blue\" becomes 2, and so on.\n",
    "This method is simple but can introduce ordinal relationships where none exist.\n",
    "\n",
    "One-Hot Encoding: Creates binary columns for each category. \n",
    "For example, a \"color\" variable with values \"red,\" \"blue,\" and \"green\" would be transformed\n",
    "into three columns: \"color_red,\" \"color_blue,\" and \"color_green,\" with binary values indicating the presence of each color.\n",
    "\n",
    "Ordinal Encoding: Similar to label encoding but used when the categories have a meaningful order.\n",
    "For example, \"low,\" \"medium,\" and \"high\" could be encoded as 1, 2, and 3, respectively.\n",
    "\n",
    "Binary Encoding: Combines label encoding and one-hot encoding. Each category is first converted to a binary number, and then each digit becomes a separate column. This method can be more memory-efficient than one-hot encoding.\n",
    "\n",
    "Frequency Encoding: Replaces categories with their frequency of occurrence. \n",
    "For example, if \"red\" appears 50 times, \"blue\" 30 times, and \"green\" 20 times, they would be encoded as 50, 30, and 20, respectively.\n",
    "\n",
    "Target Encoding: Replaces categories with the mean of the target variable for each category. \n",
    "This method can be powerful but may lead to overfitting if not used carefully.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38241c80-5efb-4ef8-80bd-53d6d13dd1e6",
   "metadata": {},
   "source": [
    "Q7. What do you mean by training and testing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da36dad5-98d9-4241-96d2-3f662f13c184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" In machine learning, training and testing a dataset are crucial steps to build and evaluate a model:\\n\\nTraining Dataset: This is the portion of the data used to train the model. \\nThe model learns patterns, relationships, and features from this data. \\nThe goal is to adjust the model's parameters to minimize errors and improve accuracy.\\n\\nTesting Dataset: This is a separate portion of the data used to evaluate the model's performance. \\nIt helps to assess how well the model generalizes to new, unseen data. The testing dataset provides\\nan unbiased evaluation of the model's accuracy and effectiveness.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' In machine learning, training and testing a dataset are crucial steps to build and evaluate a model:\n",
    "\n",
    "Training Dataset: This is the portion of the data used to train the model. \n",
    "The model learns patterns, relationships, and features from this data. \n",
    "The goal is to adjust the model's parameters to minimize errors and improve accuracy.\n",
    "\n",
    "Testing Dataset: This is a separate portion of the data used to evaluate the model's performance. \n",
    "It helps to assess how well the model generalizes to new, unseen data. The testing dataset provides\n",
    "an unbiased evaluation of the model's accuracy and effectiveness.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf3c0b-2fd8-407e-abf7-ba809e01c4ee",
   "metadata": {},
   "source": [
    "Q8. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ab8a53b-af6b-45c3-87ab-99233486e757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sklearn.preprocessing is a module in the scikit-learn library that provides various utilities for preprocessing and transforming data.\\nPreprocessing is a crucial step in machine learning, as it helps to clean, normalize, and prepare data for modeling.\\n\\nStandardScaler: Standardizes features by removing the mean and scaling to unit variance.\\n\\nMinMaxScaler: Scales features to a given range, usually between 0 and 1.\\n\\nLabelEncoder: Encodes categorical labels with values between 0 and the number of classes minus 1.\\n\\nOneHotEncoder: Converts categorical variables into a format that can be provided to ML algorithms to do a better job in prediction.\\n\\nBinarizer: Converts numerical features into binary values based on a threshold.\\n\\nPolynomialFeatures: Generates polynomial and interaction features.\\n\\nNormalizer: Normalizes samples individually to unit norm.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sklearn.preprocessing is a module in the scikit-learn library that provides various utilities for preprocessing and transforming data.\n",
    "Preprocessing is a crucial step in machine learning, as it helps to clean, normalize, and prepare data for modeling.\n",
    "\n",
    "StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
    "\n",
    "MinMaxScaler: Scales features to a given range, usually between 0 and 1.\n",
    "\n",
    "LabelEncoder: Encodes categorical labels with values between 0 and the number of classes minus 1.\n",
    "\n",
    "OneHotEncoder: Converts categorical variables into a format that can be provided to ML algorithms to do a better job in prediction.\n",
    "\n",
    "Binarizer: Converts numerical features into binary values based on a threshold.\n",
    "\n",
    "PolynomialFeatures: Generates polynomial and interaction features.\n",
    "\n",
    "Normalizer: Normalizes samples individually to unit norm.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfec61-263f-4480-96ca-91d73055b71a",
   "metadata": {},
   "source": [
    "Q9. What is a Test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8bcf68c-0ac6-493e-ba1d-d6577bdce7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nA test set is a portion of the dataset that is used to evaluate the performance of a machine learning model\\nafter it has been trained. It is separate from the training set, which is used to train the model. \\nThe test set provides an unbiased assessment of how well the model generalizes to new, unseen data.\\nBy comparing the model's predictions on the test set with the actual values, can determine its accuracy and effectiveness.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "A test set is a portion of the dataset that is used to evaluate the performance of a machine learning model\n",
    "after it has been trained. It is separate from the training set, which is used to train the model. \n",
    "The test set provides an unbiased assessment of how well the model generalizes to new, unseen data.\n",
    "By comparing the model's predictions on the test set with the actual values, can determine its accuracy and effectiveness.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908868f7-a3a9-4a78-a95d-dc66b5e74fe4",
   "metadata": {},
   "source": [
    "Q10. How do we split data for model fitting (training and testing) in Python?\n",
    " How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96ce7429-b715-4fc2-8c40-1b58a265cd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To split data for model fitting in Python, can use the train_test_split function from the scikit-learn library.\\nApproach to a Machine Learning Problem\\nDefine the Problem: Understand the problem you're trying to solve and the goals of the project.\\n\\nCollect Data: Gather relevant data from various sources.\\n\\nExplore and Clean Data: Perform exploratory data analysis (EDA) to understand the data, handle missing values, and remove outliers.\\n\\nFeature Engineering: Create new features, transform existing ones, and select the most relevant features.\\n\\nSplit Data: Divide the data into training and testing sets.\\n\\nChoose a Model: Select an appropriate machine learning algorithm based on the problem type (e.g., regression, classification).\\n\\nTrain the Model: Fit the model to the training data.\\n\\nEvaluate the Model: Assess the model's performance using the testing data and relevant metrics.\\n\\nTune Hyperparameters: Optimize the model by adjusting hyperparameters.\\n\\nDeploy the Model: Integrate the trained model into a real-world application.\\n\\nMonitor and Maintain: Continuously monitor the model's performance and update it as needed.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To split data for model fitting in Python, can use the train_test_split function from the scikit-learn library.\n",
    "Approach to a Machine Learning Problem\n",
    "Define the Problem: Understand the problem you're trying to solve and the goals of the project.\n",
    "\n",
    "Collect Data: Gather relevant data from various sources.\n",
    "\n",
    "Explore and Clean Data: Perform exploratory data analysis (EDA) to understand the data, handle missing values, and remove outliers.\n",
    "\n",
    "Feature Engineering: Create new features, transform existing ones, and select the most relevant features.\n",
    "\n",
    "Split Data: Divide the data into training and testing sets.\n",
    "\n",
    "Choose a Model: Select an appropriate machine learning algorithm based on the problem type (e.g., regression, classification).\n",
    "\n",
    "Train the Model: Fit the model to the training data.\n",
    "\n",
    "Evaluate the Model: Assess the model's performance using the testing data and relevant metrics.\n",
    "\n",
    "Tune Hyperparameters: Optimize the model by adjusting hyperparameters.\n",
    "\n",
    "Deploy the Model: Integrate the trained model into a real-world application.\n",
    "\n",
    "Monitor and Maintain: Continuously monitor the model's performance and update it as needed.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c91463-b5ab-4620-be04-faae3349203a",
   "metadata": {},
   "source": [
    "Q11.  Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8043d1c2-db71-497c-bc2e-ff37a523373c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Exploratory Data Analysis (EDA) is a crucial step before fitting a model to the data for several reasons:\\n\\nUnderstanding Data: EDA helps you understand the structure, distribution, and relationships within your data. This understanding is essential for selecting the right model and features.\\n\\nIdentifying Patterns: It allows you to identify patterns, trends, and anomalies in the data, which can inform feature engineering and model selection.\\n\\nDetecting Outliers: EDA helps in detecting outliers and unusual observations that might skew the model's performance.\\n\\nHandling Missing Values: It helps you identify and handle missing values appropriately, ensuring the model is trained on complete and accurate data.\\n\\nFeature Selection: EDA aids in selecting the most relevant features and eliminating redundant or irrelevant ones, improving model performance.\\n\\nData Transformation: It provides insights into necessary data transformations, such as scaling or encoding, to make the data suitable for modeling.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Exploratory Data Analysis (EDA) is a crucial step before fitting a model to the data for several reasons:\n",
    "\n",
    "Understanding Data: EDA helps you understand the structure, distribution, and relationships within your data. This understanding is essential for selecting the right model and features.\n",
    "\n",
    "Identifying Patterns: It allows you to identify patterns, trends, and anomalies in the data, which can inform feature engineering and model selection.\n",
    "\n",
    "Detecting Outliers: EDA helps in detecting outliers and unusual observations that might skew the model's performance.\n",
    "\n",
    "Handling Missing Values: It helps you identify and handle missing values appropriately, ensuring the model is trained on complete and accurate data.\n",
    "\n",
    "Feature Selection: EDA aids in selecting the most relevant features and eliminating redundant or irrelevant ones, improving model performance.\n",
    "\n",
    "Data Transformation: It provides insights into necessary data transformations, such as scaling or encoding, to make the data suitable for modeling.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d1e4a-f8c2-450f-863d-67491b93d3d2",
   "metadata": {},
   "source": [
    "Q12.What is correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3fab3560-ed2a-4e5f-8a8e-a7a160b2b5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Correlation measures the relationship between two variables. It indicates how one variable changes in response to changes in another variable. The correlation coefficient ranges from -1 to 1:\\n\\n1 indicates a perfect positive correlation, meaning both variables move in the same direction.\\n\\n0 indicates no correlation, meaning the variables do not affect each other.\\n\\n-1 indicates a perfect negative correlation, meaning the variables move in opposite directions.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Correlation measures the relationship between two variables. It indicates how one variable changes in response to changes in another variable. The correlation coefficient ranges from -1 to 1:\n",
    "\n",
    "1 indicates a perfect positive correlation, meaning both variables move in the same direction.\n",
    "\n",
    "0 indicates no correlation, meaning the variables do not affect each other.\n",
    "\n",
    "-1 indicates a perfect negative correlation, meaning the variables move in opposite directions.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e69e4-fc03-467e-b2e3-9789905b6c9b",
   "metadata": {},
   "source": [
    "Q13. What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc4a0a73-f78e-4695-b929-e1ef7bcc25cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A negative correlation means that as one variable increases, the other decreases. \\nFor example, if the number of hours spent watching TV increases, the number of hours spent \\nstudying might decrease, indicating a negative correlation between TV watching and studying.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''A negative correlation means that as one variable increases, the other decreases. \n",
    "For example, if the number of hours spent watching TV increases, the number of hours spent \n",
    "studying might decrease, indicating a negative correlation between TV watching and studying.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45dc2b2-b9d1-4b6c-b660-0bf58e32ab23",
   "metadata": {},
   "source": [
    "Q14. How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f55886a6-9814-471d-ab2b-44105ae9673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Matrix:\n",
      "           Variable1  Variable2  Variable3\n",
      "Variable1        1.0       -1.0        1.0\n",
      "Variable2       -1.0        1.0       -1.0\n",
      "Variable3        1.0       -1.0        1.0\n",
      "\n",
      "Correlation between Variable1 and Variable2:\n",
      "-0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Variable1': [1, 2, 3, 4, 5],\n",
    "    'Variable2': [5, 4, 3, 2, 1],\n",
    "    'Variable3': [2, 3, 4, 5, 6]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Calculate correlation between two specific variables\n",
    "correlation = df['Variable1'].corr(df['Variable2'])\n",
    "print(\"\\nCorrelation between Variable1 and Variable2:\")\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da32193-a38b-4e70-aa75-8de705cb46d0",
   "metadata": {},
   "source": [
    "Q15.  What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0feace25-b16e-4c02-ae5d-4e64d2cce918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCausation refers to a relationship where one event directly affects another. In other words,\\na change in one variable directly causes a change in another variable.\\n\\nCorrelation measures the strength and direction of a relationship between two variables, \\nbut it does not imply causation. Two variables can be correlated without one causing the other.\\n\\nExample:\\nCorrelation: Ice cream sales and drowning incidents are positively correlated. As ice cream sales\\nincrease, drowning incidents also increase. However, \\nthis does not mean that buying ice cream causes drowning.\\nThe underlying factor is the hot weather, which leads to more people buying ice cream and swimming, \\nthus increasing the risk of drowning.\\n\\nCausation: Smoking and lung cancer have a causal relationship. Extensive research has shown that\\nsmoking directly increases the risk of developing lung cancer.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Causation refers to a relationship where one event directly affects another. In other words,\n",
    "a change in one variable directly causes a change in another variable.\n",
    "\n",
    "Correlation measures the strength and direction of a relationship between two variables, \n",
    "but it does not imply causation. Two variables can be correlated without one causing the other.\n",
    "\n",
    "Example:\n",
    "Correlation: Ice cream sales and drowning incidents are positively correlated. As ice cream sales\n",
    "increase, drowning incidents also increase. However, \n",
    "this does not mean that buying ice cream causes drowning.\n",
    "The underlying factor is the hot weather, which leads to more people buying ice cream and swimming, \n",
    "thus increasing the risk of drowning.\n",
    "\n",
    "Causation: Smoking and lung cancer have a causal relationship. Extensive research has shown that\n",
    "smoking directly increases the risk of developing lung cancer.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b3c63-c728-4f09-af8a-464573bc8e3b",
   "metadata": {},
   "source": [
    "Q16.What is an Optimizer? What are different types of optimizers? Explain each with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04f7cf59-4e30-4378-9470-6c7766c92491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"An optimizer in machine learning is an algorithm or method used to adjust the parameters of a model\\nto minimize the loss function. The goal is to find the optimal set of parameters that result in the \\nbest performance of the model.\\n\\nTypes of Optimizers:\\nGradient Descent:\\n\\nExample: Suppose you have a linear regression model. Gradient Descent iteratively adjusts the model's\\nparameters to minimize the difference between the predicted and actual values.\\n\\n\\nStochastic Gradient Descent (SGD):\\n\\nExample: In a neural network, SGD updates the parameters for each training example,\\nrather than the entire dataset.\\n\\n\\nMini-batch Gradient Descent:\\n\\nExample: In image classification, mini-batch gradient descent updates the parameters using a small\\nbatch of images.\\n\\n\\nAdam (Adaptive Moment Estimation):\\n\\nExample: In deep learning, Adam is widely used for training complex models like convolutional neural\\nnetworks (CNNs).\\n\\n\\nRMSProp (Root Mean Square Propagation):\\n\\nExample: In recurrent neural networks (RNNs), RMSProp helps to handle the vanishing gradient problem.\\n\\n\\nAdaGrad (Adaptive Gradient Algorithm):\\n\\nExample: In natural language processing (NLP), AdaGrad can be used for training word embeddings.\\n\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''An optimizer in machine learning is an algorithm or method used to adjust the parameters of a model\n",
    "to minimize the loss function. The goal is to find the optimal set of parameters that result in the \n",
    "best performance of the model.\n",
    "\n",
    "Types of Optimizers:\n",
    "Gradient Descent:\n",
    "\n",
    "Example: Suppose you have a linear regression model. Gradient Descent iteratively adjusts the model's\n",
    "parameters to minimize the difference between the predicted and actual values.\n",
    "\n",
    "\n",
    "Stochastic Gradient Descent (SGD):\n",
    "\n",
    "Example: In a neural network, SGD updates the parameters for each training example,\n",
    "rather than the entire dataset.\n",
    "\n",
    "\n",
    "Mini-batch Gradient Descent:\n",
    "\n",
    "Example: In image classification, mini-batch gradient descent updates the parameters using a small\n",
    "batch of images.\n",
    "\n",
    "\n",
    "Adam (Adaptive Moment Estimation):\n",
    "\n",
    "Example: In deep learning, Adam is widely used for training complex models like convolutional neural\n",
    "networks (CNNs).\n",
    "\n",
    "\n",
    "RMSProp (Root Mean Square Propagation):\n",
    "\n",
    "Example: In recurrent neural networks (RNNs), RMSProp helps to handle the vanishing gradient problem.\n",
    "\n",
    "\n",
    "AdaGrad (Adaptive Gradient Algorithm):\n",
    "\n",
    "Example: In natural language processing (NLP), AdaGrad can be used for training word embeddings.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f1755-d21b-4c8d-b291-7e3f591f1268",
   "metadata": {},
   "source": [
    "Q17. What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47bc7dcf-3212-4670-bbe5-2d6cf96b2c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sklearn.linear_model is a module in the scikit-learn library that provides various linear models for regression and classification tasks. These models are based on linear relationships between the input features and the target variable. Here are some common classes and functions in sklearn.linear_model:\\n\\nLinearRegression: Implements ordinary least squares linear regression.\\n\\nRidge: Implements ridge regression, which includes L2 regularization to prevent overfitting.\\n\\nLasso: Implements Lasso regression, which includes L1 regularization to enforce sparsity in the model.\\n\\nElasticNet: Combines L1 and L2 regularization for regression.\\n\\nLogisticRegression: Implements logistic regression for binary classification tasks.\\n\\nSGDRegressor: Implements linear regression with stochastic gradient descent.\\n\\nSGDClassifier: Implements linear classifiers with stochastic gradient descent.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sklearn.linear_model is a module in the scikit-learn library that provides various linear models for regression and classification tasks. These models are based on linear relationships between the input features and the target variable. Here are some common classes and functions in sklearn.linear_model:\n",
    "\n",
    "LinearRegression: Implements ordinary least squares linear regression.\n",
    "\n",
    "Ridge: Implements ridge regression, which includes L2 regularization to prevent overfitting.\n",
    "\n",
    "Lasso: Implements Lasso regression, which includes L1 regularization to enforce sparsity in the model.\n",
    "\n",
    "ElasticNet: Combines L1 and L2 regularization for regression.\n",
    "\n",
    "LogisticRegression: Implements logistic regression for binary classification tasks.\n",
    "\n",
    "SGDRegressor: Implements linear regression with stochastic gradient descent.\n",
    "\n",
    "SGDClassifier: Implements linear classifiers with stochastic gradient descent.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09beb52c-e674-43ee-9dd7-2f051b946049",
   "metadata": {},
   "source": [
    "Q18. What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8010028-551a-4cbe-9983-0951a8d3d20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe model.fit() method in machine learning is used to train a model on a given dataset. \\nIt adjusts the model's parameters based on the input data and the corresponding target\\nvalues to minimize the error and improve the model's performance.\\n\\nArguments for model.fit()\\nX: The input data (features). This can be a NumPy array, pandas DataFrame, or similar data structure.\\n\\ny: The target values (labels). This is the output that the model is trying to predict, \\nand it should correspond to the input data.\\n\\nOther optional arguments: Depending on the model and library, there can be additional parameters such as:\\n\\nepochs: Number of times the entire dataset is passed through the model (used in neural networks).\\n\\nbatch_size: Number of samples processed before the model is updated (used in neural networks).\\n\\nvalidation_data: Data on which to evaluate the loss and any model metrics at the end of each\\nepoch (used in neural networks).\\n\\ncallbacks: List of callback functions to apply during training (used in neural networks).\\n\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The model.fit() method in machine learning is used to train a model on a given dataset. \n",
    "It adjusts the model's parameters based on the input data and the corresponding target\n",
    "values to minimize the error and improve the model's performance.\n",
    "\n",
    "Arguments for model.fit()\n",
    "X: The input data (features). This can be a NumPy array, pandas DataFrame, or similar data structure.\n",
    "\n",
    "y: The target values (labels). This is the output that the model is trying to predict, \n",
    "and it should correspond to the input data.\n",
    "\n",
    "Other optional arguments: Depending on the model and library, there can be additional parameters such as:\n",
    "\n",
    "epochs: Number of times the entire dataset is passed through the model (used in neural networks).\n",
    "\n",
    "batch_size: Number of samples processed before the model is updated (used in neural networks).\n",
    "\n",
    "validation_data: Data on which to evaluate the loss and any model metrics at the end of each\n",
    "epoch (used in neural networks).\n",
    "\n",
    "callbacks: List of callback functions to apply during training (used in neural networks).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b4512-8d58-4602-83b2-187128e30ce6",
   "metadata": {},
   "source": [
    "Q19.What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37801cd1-2f0e-4404-87e0-4da85ae8f026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The model.predict() method in machine learning is used to make predictions on new, \\nunseen data based on the model that has been trained. It takes the input data and returns the predicted values.\\n\\nArguments for model.predict()\\nX: The input data (features) for which you want to make predictions. \\nThis can be a NumPy array, pandas DataFrame, or similar data structure.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The model.predict() method in machine learning is used to make predictions on new, \n",
    "unseen data based on the model that has been trained. It takes the input data and returns the predicted values.\n",
    "\n",
    "Arguments for model.predict()\n",
    "X: The input data (features) for which you want to make predictions. \n",
    "This can be a NumPy array, pandas DataFrame, or similar data structure.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bce8dc2-4167-44a8-9717-d23594a46f85",
   "metadata": {},
   "source": [
    "Q20.What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d5c6b12-72f9-40ba-b02b-0a9e66267da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Continuous Variables: These are variables that can take any value within a given range. They are typically numerical and can be measured. Examples include height, weight, temperature, and time. Continuous variables can have an infinite number of possible values.\\n\\nCategorical Variables: These are variables that represent distinct categories or groups. They are often non-numerical and can be divided into a limited number of categories. Examples include gender, color, type of car, and brand of a product. Categorical variables can be further divided into:\\n\\nNominal Variables: Categories without any inherent order (e.g., colors: red, blue, green).\\n\\nOrdinal Variables: Categories with a specific order (e.g., rankings: first, second, third).'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Continuous Variables: These are variables that can take any value within a given range.\n",
    "They are typically numerical and can be measured. Examples include height, weight, temperature, \n",
    "and time. Continuous variables can have an infinite number of possible values.\n",
    "\n",
    "Categorical Variables: These are variables that represent distinct categories or groups. \n",
    "They are often non-numerical and can be divided into a limited number of categories. \n",
    "Examples include gender, color, type of car, and brand of a product. Categorical variables can be further divided into:\n",
    "\n",
    "Nominal Variables: Categories without any inherent order (e.g., colors: red, blue, green).\n",
    "\n",
    "Ordinal Variables: Categories with a specific order (e.g., rankings: first, second, third).'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10029d4f-2609-46bc-9e9f-ac0c9161d744",
   "metadata": {},
   "source": [
    "Q21. What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1f329790-dab6-415a-96f3-8e54e017de74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feature scaling is a technique used to normalize the range of independent variables or features \\nof data. In machine learning, it is crucial because many algorithms perform better or converge faster \\nwhen features are on a relatively similar scale and close to normally distributed.\\n\\nHow Feature Scaling Helps in Machine Learning:\\nImproves Model Performance: Algorithms like gradient descent converge faster with scaled data.\\n\\nEnhances Accuracy: Distance-based algorithms (e.g., K-Nearest Neighbors, SVM) perform better with scaled features.\\n\\nPrevents Dominance: Ensures that no single feature dominates others due to its scale.\\n\\nStabilizes Training: Helps in stabilizing the training process, especially for neural networks.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Feature scaling is a technique used to normalize the range of independent variables or features \n",
    "of data. In machine learning, it is crucial because many algorithms perform better or converge faster \n",
    "when features are on a relatively similar scale and close to normally distributed.\n",
    "\n",
    "How Feature Scaling Helps in Machine Learning:\n",
    "Improves Model Performance: Algorithms like gradient descent converge faster with scaled data.\n",
    "\n",
    "Enhances Accuracy: Distance-based algorithms (e.g., K-Nearest Neighbors, SVM) perform better with scaled features.\n",
    "\n",
    "Prevents Dominance: Ensures that no single feature dominates others due to its scale.\n",
    "\n",
    "Stabilizes Training: Helps in stabilizing the training process, especially for neural networks.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee22f4-6992-4f66-9d28-028cfc7a0f50",
   "metadata": {},
   "source": [
    " Q22.How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "47dbdc1b-feed-4f71-9866-f4a03a24b29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Scaled Data:\n",
      "[[-1.41421356 -1.41421356 -1.41421356]\n",
      " [-0.70710678 -0.70710678 -0.70710678]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.70710678  0.70710678  0.70710678]\n",
      " [ 1.41421356  1.41421356  1.41421356]]\n",
      "\n",
      "Min-Max Scaled Data:\n",
      "[[0.   0.   0.  ]\n",
      " [0.25 0.25 0.25]\n",
      " [0.5  0.5  0.5 ]\n",
      " [0.75 0.75 0.75]\n",
      " [1.   1.   1.  ]]\n",
      "\n",
      "Robust Scaled Data:\n",
      "[[-1.  -1.  -1. ]\n",
      " [-0.5 -0.5 -0.5]\n",
      " [ 0.   0.   0. ]\n",
      " [ 0.5  0.5  0.5]\n",
      " [ 1.   1.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Feature1': [1, 2, 3, 4, 5],\n",
    "    'Feature2': [10, 20, 30, 40, 50],\n",
    "    'Feature3': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standard Scaling\n",
    "standard_scaler = StandardScaler()\n",
    "df_standard_scaled = standard_scaler.fit_transform(df)\n",
    "print(\"Standard Scaled Data:\")\n",
    "print(df_standard_scaled)\n",
    "\n",
    "# Min-Max Scaling\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_minmax_scaled = minmax_scaler.fit_transform(df)\n",
    "print(\"\\nMin-Max Scaled Data:\")\n",
    "print(df_minmax_scaled)\n",
    "\n",
    "# Robust Scaling\n",
    "robust_scaler = RobustScaler()\n",
    "df_robust_scaled = robust_scaler.fit_transform(df)\n",
    "print(\"\\nRobust Scaled Data:\")\n",
    "print(df_robust_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e552fd-139f-4310-be36-eb5335a31026",
   "metadata": {},
   "source": [
    "Q23. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c0a0a951-53a4-4c5e-983b-998233c3b75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sklearn.preprocessing is a module in the scikit-learn library that provides various utilities for preprocessing and transforming data. Preprocessing is a crucial step in machine learning, as it helps to clean, normalize, and prepare data for modeling. Here are some common functions and classes in sklearn.preprocessing:\\n\\nStandardScaler: Standardizes features by removing the mean and scaling to unit variance.\\n\\nMinMaxScaler: Scales features to a given range, usually between 0 and 1.\\n\\nLabelEncoder: Encodes categorical labels with values between 0 and the number of classes minus 1.\\n\\nOneHotEncoder: Converts categorical variables into a format that can be provided to ML algorithms to do a better job in prediction.\\n\\nBinarizer: Converts numerical features into binary values based on a threshold.\\n\\nPolynomialFeatures: Generates polynomial and interaction features.\\n\\nNormalizer: Normalizes samples individually to unit norm.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sklearn.preprocessing is a module in the scikit-learn library that provides various utilities for preprocessing and transforming data. Preprocessing is a crucial step in machine learning, as it helps to clean, normalize, and prepare data for modeling. Here are some common functions and classes in sklearn.preprocessing:\n",
    "\n",
    "StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
    "\n",
    "MinMaxScaler: Scales features to a given range, usually between 0 and 1.\n",
    "\n",
    "LabelEncoder: Encodes categorical labels with values between 0 and the number of classes minus 1.\n",
    "\n",
    "OneHotEncoder: Converts categorical variables into a format that can be provided to ML algorithms to do a better job in prediction.\n",
    "\n",
    "Binarizer: Converts numerical features into binary values based on a threshold.\n",
    "\n",
    "PolynomialFeatures: Generates polynomial and interaction features.\n",
    "\n",
    "Normalizer: Normalizes samples individually to unit norm.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04734a7-2f52-4674-aa51-0981bfed3214",
   "metadata": {},
   "source": [
    "Q24. How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3bc428a2-c3af-4ed6-a844-b47519a9ad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "    Feature1  Feature2\n",
      "4         5        50\n",
      "2         3        30\n",
      "0         1        10\n",
      "3         4        40\n",
      "Testing Features:\n",
      "    Feature1  Feature2\n",
      "1         2        20\n",
      "Training Target:\n",
      " 4    500\n",
      "2    300\n",
      "0    100\n",
      "3    400\n",
      "Name: Target, dtype: int64\n",
      "Testing Target:\n",
      " 1    200\n",
      "Name: Target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Feature1': [1, 2, 3, 4, 5],\n",
    "    'Feature2': [10, 20, 30, 40, 50],\n",
    "    'Target': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df[['Feature1', 'Feature2']]\n",
    "y = df['Target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the results\n",
    "print(\"Training Features:\\n\", X_train)\n",
    "print(\"Testing Features:\\n\", X_test)\n",
    "print(\"Training Target:\\n\", y_train)\n",
    "print(\"Testing Target:\\n\", y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8cd3e7-7733-4f87-835c-000a9130df6c",
   "metadata": {},
   "source": [
    "Q25.Explain data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "42693843-d117-4df6-9a54-13488fe1ea70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data encoding is the process of converting categorical data into a numerical format that can be used\\nby machine learning algorithms. Since most machine learning models require numerical input, encoding is \\nessential for handling categorical variables. Here are some common encoding techniques'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Data encoding is the process of converting categorical data into a numerical format that can be used\n",
    "by machine learning algorithms. Since most machine learning models require numerical input, encoding is \n",
    "essential for handling categorical variables. Here are some common encoding techniques'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186a926-1bca-485c-baed-aa0ff9047cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
